{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = train_df.columns.tolist()\n",
    "print(len(train_cols), train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols = test_df.columns.tolist()\n",
    "print(len(test_cols), test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing columns in training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = [i for i in train_cols if i not in test_cols]\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical data to numeric and separate target feature for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('loan_status', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert output labels to 0 and 1\n",
    "y_train = LabelEncoder().fit_transform(train_df['loan_status'])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical data to numeric and separate target feature for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = test_df.drop('loan_status', axis=1)\n",
    "X_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.get_dummies(X_1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output labels to 0 and 1\n",
    "y_test = LabelEncoder().fit_transform(test_df['loan_status'])\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing \"ValueError: X has 93 features per sample; expecting 94\"\n",
    "# Find missing column and fill with 0\n",
    "missing_col = [i for i in X_train.columns if i not in X_test.columns]\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"debt_settlement_flag_Y\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = classifier.predict(X_test)\n",
    "array = confusion_matrix(y_true, y_pred)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_df = pd.DataFrame(array)\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(confusion_df, annot=True, annot_kws={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RandomForestClassifier model for scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {clf.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sorted(zip(X.columns, clf.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10,200)\n",
    "plt.margins(y=0.001)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RandomForestClassifier model for unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1 = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train, y_train)\n",
    "print(f'Training Score: {clf_1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {clf_1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression model on the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {classifier.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "array_1 = confusion_matrix(y_true, y_pred)\n",
    "array_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize confusion matrix for Logistic Regression for scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_df1 = pd.DataFrame(array_1)\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(confusion_df1, annot=True, annot_kws={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion (Logistic Regression vs Random Forest Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model \n",
    "<hr>\n",
    "\n",
    "### For the unscaled data, the scores for the Logistic Regression:\n",
    "- Training Data Score: 0.6485221674876848\n",
    "- Testing Data Score: 0.5253083794130158\n",
    "\n",
    "### For the scaled data, the scores for the Logistic Regression:\n",
    "- Training Data Score: 0.713136288998358\n",
    "- Testing Data Score: 0.7201190982560612\n",
    "\n",
    "### As one can see from the scores above, the best model to predict the Credit Risk is the Logistic Regression on the scaled data. The training scores and testing scores are much more closer than the unscaled data, hence allowing us to make a more accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Model\n",
    "<hr>\n",
    "\n",
    "### For unscaled data, the scores for the Random Forest Classifier Model: \n",
    "- Training Score: 1.0\n",
    "- Testing Score: 0.6180348787749894\n",
    "\n",
    "### For scaled data, the scores for the Random Forest Classifier Model: \n",
    "- Training Score: 1.0\n",
    "- Testing Score: 0.6193109315185028\n",
    "\n",
    "### As one can see from the scores above, the Random Forest Classifier won't be very efficient in predicting the Credit Risks. The differences between the scores on the scaled and unscaled data are almost identical (The difference is negligible. \n",
    "\n",
    "### In conclusion, for this exercise, Logistic Regression will be a better model to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
